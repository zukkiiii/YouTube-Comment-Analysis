{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import psycopg2\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gensim.models import Phrases\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from ProcessingLDA import LDAparamater\n",
    "from Database import database\n",
    "import Database\n",
    "from Connection import connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distribution(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,3), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = connection.connect(Database.params_dic)\n",
    "\n",
    "column_names = [\"document_no\",\"user_name\", \"comments_user\", \"video_title\", \"channel\", \"views\", \"posted\", \"comments_clean\",\n",
    "                \"comments_tokenize\", \"comments_slangword\", \"comments_stemmed\", \"comments_stopwords\", \"text_string\"]\n",
    "\n",
    "df = database.postgresql_to_dataframe(con, \"select * from comments_preprocessing\", column_names)\n",
    "\n",
    "con.close()\n",
    "\n",
    "df = df.sort_values(by=['document_no'])\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text_string']\n",
    "text_list =  [i.split() for i in text]\n",
    "\n",
    "bigram = Phrases(text_list, min_count=10)\n",
    "trigram = Phrases(bigram[text_list])\n",
    "for idx in range(len(text_list)):\n",
    "    for token in bigram[text_list[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            text_list[idx].append(token)\n",
    "    for token in trigram[text_list[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a trigram, add to document.\n",
    "            text_list[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembuatan Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(text_list)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=20000)\n",
    "dictionary.save('dictionary_comments.dict')\n",
    "\n",
    "# Pembuatan Corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in text_list]\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penentuan Jumlah Topik Optimal\n",
    "start=1\n",
    "limit=21\n",
    "step=1\n",
    "model_list, coherence_values = LDAparamater.compute_coherence_values_numtopics(bow_corpus, dictionary, text_list,\n",
    "                                                                                limit, start, step)\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \"has Coherence Value of\", round(cv, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penentuan Nilai Alpha & Beta\n",
    "alpha = list(np.arange(0.01, 1, 0.1))\n",
    "alpha.append(1)\n",
    "beta = list(np.arange(0.01, 1, 0.1))\n",
    "beta.append(1)\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=121)\n",
    "    for a in alpha:\n",
    "        for b in beta:\n",
    "            cv = LDAparamater.compute_coherence_values_hyperparameters(bow_corpus, dictionary, 6, a, b)\n",
    "               \n",
    "            model_results['Topics'].append(k)\n",
    "            model_results['Alpha'].append(a)\n",
    "            model_results['Beta'].append(b)\n",
    "            model_results['Coherence'].append(cv)\n",
    "                    \n",
    "            pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_hyperparameters_results_using_bow.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penentuan Jumlah Iterasi Optimal\n",
    "passes = list(np.arange(5, 51, 5))\n",
    "\n",
    "passes_model_results = {'Topics': [],\n",
    "                        'Alpha': [],\n",
    "                        'Beta': [],\n",
    "                        'Passes': [],\n",
    "                        'Coherence': []}\n",
    "\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=10)\n",
    "    \n",
    "    for p in passes:\n",
    "        cv = LDAparamater.compute_coherence_values_passes(bow_corpus, dictionary, 6, 0.3, 0.5, p)\n",
    "        passes_model_results['Topics'].append(k)\n",
    "        passes_model_results['Alpha'].append(a)\n",
    "        passes_model_results['Beta'].append(b)\n",
    "        passes_model_results['Passes'].append(p)\n",
    "        passes_model_results['Coherence'].append(cv)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    \n",
    "pd.DataFrame(passes_model_results).to_csv('lda_tuning_hyperparameters_results_using_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL MODEL\n",
    "lda_model_final = gensim.models.LdaMulticore(corpus=bow_corpus, id2word=dictionary, num_topics=6, alpha=0.3, eta=0.5, passes=30, workers=2)\n",
    "coherence_model_lda_final = CoherenceModel(model=lda_model_final, texts=text_list, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "for idx, topic in lda_model_final.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_lda_final = coherence_model_lda_final.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_final)\n",
    "\n",
    "lda_model_final.save('lda_model_final.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis = pyLDAvis.gensim.prepare(lda_model_final, bow_corpus, dictionary)\n",
    "\n",
    "LDAvis\n",
    "\n",
    "LDAvis.topic_info.to_csv(Database.path_data + 'LDA_term_detail.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi Tiap Komentar\n",
    "df_topic_sents_keywords = topic_distribution(ldamodel=lda_model_final, corpus=bow_corpus, texts=text_list)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.to_csv(Database.path_data + 'Distribution Topics Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat ke Database\n",
    "database.createTableDatabaseTD()\n",
    "database.intoDatabaseTD()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
